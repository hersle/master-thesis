\appendix

\chapter{General relativity}

\textit{This appendix is inspired by references \cite{ref:carroll}, \cite{ref:mtw} and \cite{ref:mika_gr_notes}.}

\section{The geometry of curved spacetime}
\label{chap:gr_summary} % TODO: chap -> sec

\newcommand\pdvx[2]{\pdv{x^{#1}}{x^{#2}}}

In this section, we review the geometrical aspects of general relativity.
We make no attempt to be mathematically rigorous, but rather focus on listing important quantities and equations and the intuitive connection between them.
For more details, we refer to the references listed above which this summary is based on.

\subsection{Coordinates and tensors}

In general relativity, $3$-dimensional space and $1$-dimensional time are no longer regarded separate as they are in Newtonian mechanics.
They are rather intertwined into \emph{spacetime} -- a $(3+1)$-dimensional construct with \textbf{coordinates}
% position is just "coordinates", not a 4-vector: https://physics.stackexchange.com/questions/192886/does-spacetime-position-not-form-a-four-vector
\begin{equation}
	x^\mu = (x^0, x^1, x^2, x^3) .
\end{equation}
In flat Minkowski space, the coordinates could be taken as $x^\mu = (ct, x, y, z)$.
Mathematically, the geometry of spacetime is described by a \emph{Riemannian manifold} that generalizes flat Minkowski space to \emph{curved space}.
At every point on such a manifold, spacetime locally resembles Minkowski space in the \emph{tangent space} located at that point, and all such tangent spaces vary in a smooth manner from point to point.
For example, \cref{fig:tangent_space} pictures the tangent space at a point of the $2$-sphere manifold.
Familiar concepts like angles, lengths, area and volume apply locally in the tangent space at each point in infinitesimal form, and one can generalize such concepts to the full manifold by integrating the local contributions from one point on the manifold to another.

\begin{figure}
\centering
\includesvg[width=0.60\textwidth]{figures/tangent-space.svg}
\caption{\label{fig:tangent_space}The tangent space at a point on the $2$-sphere manifold can be pictured as the tangent plane at that point. If a vector field is placed on the manifold, the vector would lie in this tangent space. Illustration by \cite{ref:figure_tangent_space}.}
\end{figure}

% transformation law inspiration: https://math.stackexchange.com/a/958524 and Wikipedia: "holonomic basis"

\newcommand\lincombo[1]{\left( a \odv{#1}{\tau} + b \odv{#1}{\lambda} \right)}
\newcommand\lincomboslash[1]{\left( a \odv{#1}/{\tau} + b \odv{#1}/{\lambda} \right)}
We will place vector fields $V^\mu(x^\nu)$, and later tensor fields, that associate a vector $V^\mu$ to every point $x^\nu$ on a manifold.
As explained in \cref{fig:tangent_space}, such a vector lies in the tangent vector space at every point on the manifold.
To motivate the transformation properties of tensors on a manifold, we can use the fact that the set of directional derivatives constitute a vector space with basis vectors given by the partial derivatives. 
Suppose $\phi(x)$ is a scalar function and $x(\tau)$ and $x(\lambda)$ are two paths on the manifold with directional derivatives given through the chain rule as
\begin{equation}
	\odv{\phi}{\tau} = \odv{x^\mu}{\tau} \partial_\mu \phi
	\qquad \text{and} \qquad
	\odv{\phi}{\lambda} = \odv{x^\mu}{\lambda} \partial_\mu \phi .
\end{equation}
Then the linear combination $\lincomboslash{}$ is also a perfectly good derivative operator, as it is both linear and satisfies the product rule
\begin{equation}
	\lincombo{}(fg) = \ldots = \lincombo{f} g + \lincombo{g} f .
\end{equation}
One can verify that the set of all differential operators, implicitly assumed to work on some scalar function, satisfy all criteria for being a vector space.
Thus, like one can regard $\vec{V} = V^\mu \vec{e}_\mu$ as a vector with components $V^\mu$ and basis vectors $\vec{e}_\mu$, one can regard
\begin{equation}
	\odv{}{\lambda} = \odv{x^{\mu}}{\lambda} \partial_{\mu} 
\end{equation}
as a vector with components $\odv{x^{\mu}}/{\lambda}$ and basis vectors $\partial_\mu$.
To see this clearly, make a coordinate transformation
\begin{equation}
	x \rightarrow x'(x)
	\qquad \text{with inverse} \qquad
	x' \rightarrow x(x')
	\qquad \text{and Jacobians} \qquad
	\pdv{x'^{\alpha}}{x^\gamma} \pdv{x^{\gamma}}{x'^{\beta}} = \delta^{\alpha}_{\beta} .
	\label{eq:gr_summary:coordinate_transformation}
\end{equation}
Using the chain rule and the Jacobian property, the directional derivative transforms as
\begin{equation}
	\odv{}{\lambda} = \odv{x^{\mu'}}{\lambda} \partial_{\mu'} = \underbrace{\Bigg( \odv{x^{\alpha}}{\lambda} \pdvx{\mu'}{\alpha} \Bigg)}_\text{components} \underbrace{\Bigg( \pdvx{\beta}{\mu'} \partial_{\beta} \Bigg)}_\text{basis} = \odv{x^{\mu}}{\lambda} \partial_{\mu} .
\end{equation}
We see that the transformation of the components and the basis vectors exactly cancel each other, so the directional derivative is unchanged -- consistent with it being a vector.

Note that we adopted the convention $x^{\mu'} = x'^\mu$ of placing the prime on the \emph{index} rather than the underlying object, but defined to mean the same.
The benefit is that an index transforms with the partial derivative that has the primed coordinate in the \emph{same position as the index}, so we can always deduce the right transformation by simply staring at the expression.
%If there is a prime on an index, it means that a coordinate transformation has been applied to that index.
%The benefit of this is that an index transforms with the partial derivative that has the primed coordinate in the \emph{same position as the index} -- upper indices go with primed coordinates in the numerator, and lower indices with primes in the denominator.
%In addition, we are free to ``reuse'' the same index twice -- once as a free index, and once for the contraction.
%With this convention, we can always deduce the correct transformation laws from the index position, and we can place remaining indices so that they are contracted to yield an expression with the right number of free indices.

We define an $n$-dimensional \textbf{covariant vector} as an $n$-tuple $V_\mu$ that transforms with the \emph{same} matrix $\pdv{x^{\mu}}/{x^{\mu'}}$ as the change of basis as
\begin{subequations}
\begin{equation}
	V_{\mu'} = \pdv{x^{\mu}}{x^{\mu'}} V_\mu .
	\label{eq:covariant_transformation}
\end{equation}
Oppositely, we define an $n$-dimensional \textbf{contravariant vector} as an $n$-tuple $V^\mu$ that transforms with the \emph{inverse} matrix $\pdv{x^{\mu'}}/{x^{\mu}}$ as
\begin{equation}
	V^{\mu'} = \pdv{x^{\mu'}}{x^\mu} V_\mu .
	\label{eq:contravariant_transformation}
\end{equation}
More generally, we define a $n$-dimensional \textbf{tensor} of rank $(r,s)$ as an array composed of $r$ $n$-dimensional contravariant indices and $s$ $n$-dimensional covariant indices that transforms as
\begin{equation}
	T^{\mu_1' \dots \mu_r'}_{\nu_1' \dots \nu_s'} = \pdv{x^{\mu_1'}}{x^{\mu_1}} \cdots \pdv{x^{\mu_r'}}{x^{\mu_r}}
	                                                \pdv{x^{\nu_1}}{x^{\nu_1'}} \cdots \pdv{x^{\nu_s}}{x^{\nu_s'}}
												    T^{\mu_1 \dots \mu_r}_{\nu_1 \dots \nu_s}
	\label{eq:tensor_transformation}
\end{equation}
under the coordinate transformation \eqref{eq:gr_summary:coordinate_transformation}.
\end{subequations}

\iffalse
We will work with vector fields $V^\mu(x)$ on manifolds.
To each point on $x$ on the manifold, we associate a tangent vector $V^\mu(x)$.
How do vectors, and generally tensors, transform under a change of coordinates $x' = x'(x)$ on the manifold?
First, note that \emph{directional derivatives} constitute a vector space when acting on scalar functions.
Imagine two curves $x^\mu(\tau)$ and $x^\nu(\lambda)$ with directional derivatives $\odv{}/{\tau}$ and $\odv{}/{\lambda}$.
The linear combination $a \odv{}/{\tau} + b \odv{}/{\lambda}$ is also in the same space, as
\begin{equation}
	\lincombo{}(fg) = \ldots = \lincombo{f} g + \lincombo{g} f
\end{equation}
so the Leibniz rule is satisfied and the linear combination is also a proper derivative operator.
By the chain rule,
\begin{equation}
	\odv{}{\lambda} = \odv{x^\mu}{\lambda} \partial_\mu ,
\end{equation}
so the partial derivatives $\partial_\mu$ in fact constitute a natural basis for this vector space.
Since these are the basis vectors, we can deduce the transformation laws by requiring that $V^\mu \partial_\mu$ be constant under a change of coordinates:
\begin{equation}
	V^\mu \partial_\mu = V^{\mu'} \partial_{\mu'} = V^{\mu'} \pdv{x^\mu}{x^{\mu'}} \partial_\mu ,
\end{equation}
so the general \textbf{transformation law for contravariant vectors} is
\begin{equation}
	V^{\mu'} = \pdv{x^{\mu'}}{x^\mu} V_\mu .
\end{equation}

To get the transformation law for covariant vectors, we make use of the gradient of a scalar function
\begin{equation}
	\nabla \phi = \partial_\mu \phi \vec{e}^\mu 
\end{equation}
Using the chain rule,
\begin{equation}	
	\partial_{\mu'} \phi = \pdv{x^\mu}{x^{\mu'}} \partial_\mu \phi ,
\end{equation}
so the \textbf{transformation law for covariant vectors} is
\begin{equation}
	V_{\mu'} = \pdv{x^{\mu}}{x^{\mu'}} V_\mu .
\end{equation}
This line of reasoning can be extended to find the \textbf{general transformation law for tensors} (where we suppress the ordering of the indices)
\begin{equation}
	T^{\mu_1' \dots \mu_m'}_{\nu_1' \dots \nu_n'} = \pdv{x^{\mu_1'}}{x^{\mu_1}} \cdots \pdv{x^{\mu_m'}}{x^{\mu_m}}
	                                                \pdv{x^{\nu_1'}}{x^{\nu_1}} \cdots \pdv{x^{\nu_n'}}{x^{\nu_n}}
												    T^{\mu_1 \dots \mu_m}_{\nu_1 \dots \nu_n}
\end{equation}
\fi

\subsection{Metric tensor}

The \textbf{metric tensor}
\begin{equation}
	g\indices{_\mu_\nu}(x) = \vec{e}_\mu(x) \cdot \vec{e}_\nu(x)
\end{equation}
is defined as the inner products between basis vectors $\vec{e}_\mu$ that span the tangent spaces at each point $x$ on the manifold.
It thus encodes the lengths vectors of the basis vectors and angles between them and is a fundamental object that describes the geometry of the manifold.

We define the \textbf{inverse metric tensor} $g^{\mu \nu}$ as the inverse matrix satisfying
\begin{equation}
	g^{\mu \nu} g_{\nu \sigma} = \delta^\mu_\sigma .
\end{equation}
Using the metric and its inverse, we can \textbf{raise and lower indices} on tensors.
For example, the object $g_{\mu \nu} V^\nu$, according to definition \eqref{eq:tensor_transformation}, transforms as a covariant vector
\begin{equation}
	g_{\mu' \nu'} V^{\nu'} = \left( \pdvx{\alpha}{\mu'} \pdvx{\beta}{\nu'} g_{\alpha \beta} \right) \left( \pdvx{\nu'}{\nu} V^{\nu} \right) = \pdvx{\alpha}{\mu'} g_{\alpha \nu} V^\nu ,
\end{equation}
so it is meaningful to label it as a covariant vector with a lower index $V_\mu = g_{\mu \nu} V^\nu$.
Similarly, we can use the inverse metric $g^{\mu \nu}$ to raise indices.
As this argument only relied on the defining transformation law \eqref{eq:tensor_transformation}, it is clear that any tensor of rank $(2,0)$ or $(0,2)$ would suffice to raise or lower indices.
But the metric is the most natural choice, as it is inherent to the manifold and always available to us.

\subsection{Line element and volume}

From the metric tensor, one defines the \textbf{line element}
\begin{subequations}
\begin{equation}
	\dif s = \sqrt{g\indices{_\mu_\nu} \dif x^\mu \dif x^\nu}
	\label{eq:def_line_elem}
\end{equation} 
that extends the concept of distance locally to every point on the manifold.
By integrating the line element from one point on the manifold to another, one can compute the total distance
\begin{equation}
	s = \int_1^2 \dif s = \int_1^2 \sqrt{g\indices{_\mu_\nu} \dif x^\mu \dif x^\nu}
\end{equation}
between the points.
Similarly, one can compute the \textbf{volume} of a region
\begin{equation}
	V = \int \dif V = \int \sqrt{-\det{\gamma}} \dif x^1 \dif x^2 \dif x^3 ,
\end{equation}
\end{subequations}
where $\gamma$ is the induced metric on the surface and $\det{\gamma} < 0$ its determinant.
The factor $\sqrt{-\det{\gamma}}$ arises to make the volume element $\dif^n x \sqrt{-\det{\gamma}}$ invariant under coordinate transformations.

\subsection{Covariant derivatives and connection coefficients}

Knowing how vectors and general tensors transform, let us generalize the notion of a derivative to curved space.
By the transformation rules we have found so far, the normal partial derivative of a vector transforms as
\begin{equation}
	\partial_{\mu'} V^{\nu'} = \bigg( \pdvx{\mu}{\mu'} \partial_\mu \bigg) \bigg( \pdvx{\nu'}{\nu} V^\nu \bigg)
	                         = \pdvx{\mu}{\mu'} \pdvx{\nu'}{\nu} \partial_\mu V^\nu + \pdvx{\mu}{\mu'} \pdv{x^{\nu'}}{x^\mu, x^\nu} V^\nu .
\end{equation}
The first term respects the tensor transformation law \eqref{eq:tensor_transformation}, but the second does not, so $\partial_\mu V^\nu$ is \emph{not} a tensor.
We define a tensorial derivative $\nabla_\mu$ that by demanding that it transforms as
\begin{equation}
	\nabla_{\mu'} V^{\nu'} = \pdvx{\mu}{\mu'} \pdvx{\nu'}{\nu} \nabla_\mu V^\nu .
\label{eq:gr_summary:covariant_derivative_demand}
\end{equation}
It turns out that our requirements can be met if we define the \textbf{covariant derivative} as
\begin{equation}
	\nabla_\mu V^\nu = \partial_\mu V^\nu + \Gamma_{\sigma \mu}^\nu V^\sigma .
	%\qquad \text{or} \qquad
	%\nabla_\mu V_\nu = \partial_\mu V_\nu - \Gamma_{\nu \mu}^\sigma V_\sigma ,
\end{equation}
It is possible to show that it obeys the tensorial transformation \eqref{eq:gr_summary:covariant_derivative_demand} if the so-called \textbf{connection coefficients} $\Gamma_{\sigma _\mu}^\nu$ transform according to \cite[equation 3.6-3.10]{ref:carroll} 
\begin{equation}
	\Gamma^{\nu'}_{\mu' \lambda'} = \pdvx{\mu}{\mu'} \pdvx{\lambda}{\lambda'} \pdvx{\nu'}{\nu}  \Gamma^{\nu}_{\mu \lambda} + \pdvx{\mu}{\mu'} \pdvx{\lambda}{\lambda'} \pdv{x^{\nu'}}{x^\mu, x^\lambda} .
	\label{eq:connection_transformation}
\end{equation}
It is possible to generalize the covariant derivative to an arbitrary tensor $T^{\alpha_1 \ldots \alpha_r}_{\beta_1 \ldots \beta_s}$ of rank $(r,s)$ (where we suppress the order of the indices) by adding more terms with connection coefficients.
The general \textbf{covariant derivative} that respects the tensorial transformation law \eqref{eq:tensor_transformation} turns out to be \cite[equation 3.11-3.16]{ref:carroll}
\begin{equation}
\begin{split}
	\nabla_\mu T^{\alpha_1 \ldots \alpha_r}_{\beta_1 \ldots \beta_s} &= \partial_\mu T^{\alpha_1 \ldots \alpha_r}_{\beta_1 \ldots \beta_s} \\
	                                                                 &+ \Gamma^{\alpha_1}_{\sigma\mu} T^{\sigma \alpha_2 \ldots \alpha_r}_{\beta_1 \ldots \beta_s} + \dots + \Gamma^{\alpha_r}_{\sigma\mu} T^{\alpha_1 \ldots \alpha_{r-1}\sigma}_{\beta_1 \ldots \beta_s} \\
	                                                                 &- \Gamma^\sigma_{\beta_1 \mu} T^{\alpha_1 \ldots \alpha_r}_{\sigma \beta_2 \ldots \beta_s} - \cdots - \Gamma^\sigma_{\beta_s \mu} T^{\alpha_1 \ldots \alpha_r}_{\beta_1 \ldots \beta_{s-1} \sigma} .
	\label{eq:def_cov_deriv}
\end{split}
\end{equation}
That is, for each upper index $\alpha_i$, add $+\Gamma^{\alpha_i}_{\sigma \mu} T^{\alpha_1 \ldots \alpha_{i-1} \sigma \alpha_{i+1} \ldots \alpha_r}_{\beta_1 \ldots \beta_s}$,
and for each lower index $\beta_i$, add $-\Gamma^{\sigma}_{\beta_i \mu} T^{\alpha_1 \ldots \alpha_r}_{\beta_1 \ldots \beta_{i-1} \sigma \beta_{i+1} \ldots \beta_s}$,

Note that the connection coefficients \eqref{eq:connection_transformation} \emph{do not} transform like tensors -- the whole point is to stash the non-tensorial behavior into the connection coefficients so that the \emph{covariant derivative} transforms as a tensor.
However, since $\nabla_\mu V^\nu$ and $\hat{\nabla}_\mu V^\nu$ for two different connection coefficients $\Gamma^\alpha_{\beta \mu}$ and $\hat{\Gamma}^\alpha_{\beta \mu}$ by definition are tensors, the \emph{difference}
\begin{equation}
	S\indices{^\alpha_\beta_\gamma} = \Gamma^\alpha_{\beta \gamma} - \hat{\Gamma}^\alpha_{\beta \gamma}
\end{equation}
between two connection coefficients \emph{does} transform like a tensor.

There are many possible choices of the connection coefficients that satisfy \cref{eq:connection_transformation}.
However, it turns out that we can find a set of \emph{unique} connection coefficients from the \emph{metric} if we impose two additional requirements.
First, we demand that the \textbf{torsion tensor}
\begin{equation}
	T\indices{^\alpha_\beta_\gamma} = \Gamma^\alpha_{\beta\gamma} - \Gamma^\alpha_{\gamma\beta}
	\label{eq:torsion_tensor}
\end{equation}
vanishes.
Equivalently, the connection coefficients $ \Gamma^\alpha_{\beta\gamma} = \Gamma^\alpha_{\beta\gamma} $ are symmetric in the lower indices.
Second, we require \textbf{metric compatibility}
\begin{equation}
	\nabla_\rho g_{\mu \nu} = 0 ,
\end{equation}
expressing that the metric is covariantly constant.
One can show that this guarantees that lengths of vectors and angles between them are preserved under parallel transport, which we will study in the next section, making this a reasonable demand \cite[equation 2.10]{ref:hehl}.
The covariantly constant nature of the metric implies that we can view spacetime as a continuum of flat Minkowski spacetimes, sewn together by the connection $\Gamma^\alpha_{\beta \gamma}$.
Metric compatibility implies that
\begin{equation}
	g_{\mu \lambda} \nabla_\rho V^\lambda = \nabla_\rho (g_{\mu \lambda} V^\lambda) = \nabla_\rho V_\mu ,
\end{equation}
so we can raise and lower indices inside covariant derivatives, even if the metric is outside.
Using both of these assumptions, we can write out three metric compatibility requirements
\newcommand\metriccompatibilityequation[3]{\nabla_{#1} g_{{#2}{#3}} &= \partial_{#1} g_{{#2}{#3}} - \Gamma^\lambda_{{#1}{#2}} g_{\lambda {#3}} - \Gamma^\lambda_{{#1}{#3}} g_{{#2} \lambda} = 0}
\begin{subequations}
\begin{align}
	\metriccompatibilityequation{\rho}{\mu}{\nu} , \\
	\metriccompatibilityequation{\mu}{\nu}{\rho} , \\
	\metriccompatibilityequation{\nu}{\rho}{\mu} .
\end{align}
\end{subequations}
By subtracting the second and third equation from the first and solving the resulting equation for the connection, we find the \textbf{Christoffel symbols} or \textbf{metric connection}
\begin{equation}
	\Gamma^\sigma_{\mu \nu} = \frac{1}{2} g\indices{^\sigma^\rho} \left(
		\partial\indices{_\mu} g\indices{_\nu_\rho} +
		\partial\indices{_\nu} g\indices{_\rho_\mu} -
		\partial\indices{_\rho} g\indices{_\mu_\nu}
	\right) .
	\label{eq:def_christoffel}
\end{equation}
In general relativity, we will \emph{always} use this unique representation of the connection coefficients given in terms of the metric only.
With this choice, the metric single-handedly determines the geometry of spacetime, as it is the only fundamental object that all the other geometric quantities we have looked at depends on.
In fact, the requirements of zero torsion and metric compatibility that led to this metric can be viewed as defining features of general relativity.
By relaxing either of these requirements, one can come up with various generalizations of general relativity.
For example, by allowing for nonzero torsion \eqref{eq:torsion_tensor}, one obtains the \emph{Einstein-Cartan theory} of gravitation.
One can show that the inclusion of torsion is equivalent to accounting for the \emph{spin} of the microscopic particles that make up the macroscopic matter \cite{ref:hehl}.
However, general relativity is concerned with describing \emph{macroscopic} objects, which justifies our demand of zero torsion.

\iffalse
\begin{align}
	\nabla_c T\indices{^{a_1 \ldots a_r}_{b_1 \ldots b_s}} &= \partial_c {T^{a_1 \ldots a_r}}_{b_1 \ldots b_s} \\
	                                                       &+ \Gamma^{a_1}_{dc} T\indices{^{d a_2 \ldots a_r}_{b_1 \ldots b_s}} + \dots + \Gamma^{a_r}_{dc} T\indices{^{a_1 \ldots a_{r-1}d}_{b_1 \ldots b_s}} \\
	                                                       &- {\Gamma^d}_{b_1 c} {T^{a_1 \ldots a_r}}_{d b_2 \ldots b_s} - \cdots - {\Gamma^d}_{b_s c} {T^{a_1 \ldots a_r}}_{b_1 \ldots b_{s-1} d}.
	\label{eq:def_cov_deriv}
\end{align}
\fi

\subsection{Parallel transport and geodesic equation}
\label{sec:geodesic}

\begin{figure}
\centering
\includesvg[width=0.5\textwidth]{figures/parallel_transport.svg}
\caption{\label{fig:parallel_transport}When a vector is parallel transported around a closed loop on the $2$-sphere, its final direction depends on the path taken. Illustration by \cite{ref:figure_parallel_transport}. \TODO{more relevant figure}}
\end{figure}

Now that we know how to take proper derivatives of vector fields and general tensor fields on manifolds, we can discuss how to parallel transport vectors on the manifold.
In flat space, we can move a vector around and keep its Cartesian components constant to parallel transport it.
But on a curved $2$-sphere, a vector that is parallel transported will end up being different depending on the route taken, as illustrated in \cref{fig:parallel_transport}.
Generalizing the directional derivative $\odv{}/{\tau} = (\odv{x^\mu}/{\tau}) \partial_\mu$ from calculus, we define the \textbf{directional covariant derivative} along a path $x(\tau)$ as
\begin{equation}
	\frac{D}{\mathrm{d} \tau} = \odv{x^\mu}{\tau} \nabla_\mu,
\end{equation}
where $\nabla_\mu$ is the covariant derivative \eqref{eq:def_cov_deriv}.
We say that a tensor is parallel transported if its components are kept constant during transport, as expressed by
\begin{equation}
	\frac{D}{\mathrm{d} \tau} T^{\mu_1 \ldots \mu_m}_{\nu_1 \ldots \nu_n} = 0 .
\end{equation}
For the special case of a vector $V^\mu$ we get the \textbf{equation of parallel transport}
\begin{equation}
	\odv{V^\mu}{\tau} + \Gamma^{\mu}_{\sigma \rho} \odv{x^\sigma}{\tau} V^\rho = 0 .
	\label{eq:parallel_transport}
\end{equation}
The solution of this first order differential equation is the continuation $V^\mu(\tau)$ from an initial vector $V^\mu(0)$ along the path such that its components are constant.

In Euclidean space, a straight line is the shortest path between two points.
In curved space, we call the shortest path between two points on a manifold a \textbf{geodesic}.
An equivalent definition of both a straight line and a geodesic is that it is the path $x(\tau)$ that parallel transports its own tangent vector $\odv{x^\mu}/{\tau}$.
Inserted into the equation of parallel transport \eqref{eq:parallel_transport}, we find the \textbf{geodesic equation}
\begin{equation}
	\odv[2]{x^\mu}{\tau} + \Gamma^\mu_{\rho \sigma} \odv{x^\rho}{\tau} \odv{x^\sigma}{\tau} = 0 .
	\label{eq:geodesic}
\end{equation}
In flat space, it reduces to the equation of a straight line $\odv[2]{x^\mu}/{\tau} = 0$.
One of Einstein's profound insights of general relativity was that gravity does not simply alter the path of a freely falling particle away from the straight line it would follow in Euclidean space in the abscence of gravity.
Instead, gravity presents itself in the geometry of spacetime, as the presence of energy-momentum curves spacetime and lays geodesic ``tracks'' according to \cref{eq:geodesic} that any freely falling particle is destined to follow.
\emph{Gravity is geometry}.

\subsection{Riemann curvature tensor, Ricci tensor and Ricci scalar}

% motivate by connecting it to the metric? 
% https://math.stackexchange.com/a/1213124 
% https://math.stackexchange.com/q/884794 
% https://math.stackexchange.com/q/2896648
% https://en.wikipedia.org/wiki/Ricci_curvature#Direct_geometric_meaning (taylor expansion around normal coords) 

So far we have used the term ``curvature'' quite informally -- let us now formalize this.
We already saw that parallel transporting a vector along different paths on a curved manifold like the $2$-sphere yield different results.
We have also seen that the covariant derivative measures the rate of change of a vector along some direction compared to what it would've been if it was parallel transported.
Thus, the commutator $[ \nabla_\mu, \nabla_\nu ] V^\rho = \nabla_\mu V^\rho - \nabla_\nu V^\rho$ measures the difference of parallel transporting a vector along the two different directions.
Using \cref{eq:def_cov_deriv}, it turns out we can write this as
\begin{equation}
	[ \nabla_\mu, \nabla_\nu ] V^\rho = R\indices{^\rho_{\sigma \mu \nu}} V^\sigma - T\indices{^\lambda_{\mu \nu}} \nabla_\lambda V^\rho ,
\end{equation}
where $T\indices{^\lambda_{\mu \nu}}$ is the torsion tensor \eqref{eq:torsion_tensor} that we assume to vanish and we define the \textbf{Riemann curvature tensor}
\begin{equation}
	R\indices{^\rho_\sigma_\mu_\nu} =
	\partial\indices{_\mu} \Gamma^\rho_{\nu \sigma} -
	\partial\indices{_\nu} \Gamma^{\rho}_{\mu \sigma} +
	\Gamma^\rho_{\mu \lambda} \Gamma^{\lambda}_{\nu \sigma} -
	\Gamma^\rho_{\nu \lambda} \Gamma^{\lambda}_{\mu \sigma} .
	\label{eq:def_riemann_tensor}
\end{equation}
We expect that if space is flat, then a parallel transported vector should not depend on the path, so the commutator and thus the Riemann tensor should vanish.
If there exists \emph{any} choice of coordinates in which the curvature tensor vanishes, then it vanishes in \emph{all} coordinates by its tensorial nature, and this is our ultimate definition of \textbf{flat space}.
In fact, it turns out that at any point $x_0$ we can find \textbf{normal coordinates} $x^\mu$ in which the metric locally resembles Minkowski space with $g_{\mu \nu} = \eta_{\mu \nu}$ and $\partial_\sigma g_{\mu \nu} = 0$ to first order in the displacement.
To second order in the displacement, \cite{ref:metric_taylor_expansion} shows that the metric can be written
\begin{equation}
	g_{\mu \nu}(x) = \eta_{\mu \nu} - \frac12 R_{\alpha \mu \beta \nu} (x^\alpha - x_0^\alpha) (x^\beta - x_0^\beta) .
\end{equation}
This shows that the Riemann tensor is a \emph{very} appropriate measure of curvature.

% TODO: motivate
% https://en.wikipedia.org/wiki/Introduction_to_the_mathematics_of_general_relativity#Curvature_tensor (need two indices to enter the Einstein field equations -- "geometry" = metric comes with 2 indices, and "energy-momentum" come with 2 indices, https://physics.stackexchange.com/a/220650)
% see great motivation at https://physics.stackexchange.com/a/220650
% and great motivation at https://physics.stackexchange.com/a/219682

From the curvature tensor, we can form tensors of lower rank by contracting some of its indices.
We know that the energy-momentum tensor $T^{\mu \nu}$ that enter the Einstein field equations \eqref{eq:einstein} are of second rank, so if it is to determine the curvature of spacetime by a tensor equation, then the curvature tensor must be contracted to form a tensor of equal rank.
With the Christoffel connection \cref{eq:def_christoffel}, it turns out that the only independent contraction we can make is the \textbf{Ricci tensor}
\begin{equation}
	R\indices{_\mu_\nu} = R\indices{^\lambda_\mu_\lambda_\nu} .
	\label{eq:def_ricci_tensor}
\end{equation}
The two other possible contractions either vanish or are related to the Ricci tensor.
Thus, the simplest scalar quantity we can form that represents curvature is the \textbf{Ricci scalar}
\begin{equation}
	R = R\indices{^\mu_\mu} .
	\label{eq:def_ricci_scalar}
\end{equation}

\subsection{Example: \texorpdfstring{$2$}{2}-sphere}

\TODO{do $2$-sphere as an example, connect it to a figure with sphere and parallell transport?}

As a simple example and to make sense of some of our results, let us study the geometry of the $2$-sphere of constant radius $r$ with coordinates
\begin{equation}
	(x^1, x^2) = (\theta, \phi)
	\quad \text{with} \quad
	0 \leq \theta < \pi , \quad
	0 \leq \phi < 2 \pi ,
\end{equation}
and the line element and metric given by
\begin{equation}
	\dif s^2 = r^2 \dif \theta^2 + r^2 \sin^2 \theta .
\end{equation}

The non-zero Christoffel symbols \eqref{eq:def_christoffel} are
\begin{equation}
	\Gamma^1_{22} = -\sin \theta \cos \theta , \qquad
	\Gamma^2_{12} = \Gamma^2_{21} = \frac{\cos \theta}{\sin \theta} .
\end{equation}

The equation of parallel transport \eqref{eq:parallel_transport} of a vector with components $(V^\theta, V^\phi)$ is
\begin{equation}
	\dot{V}^\theta - \sin\theta \cos\theta \, \dot\phi V^\phi = 0,
	\quad
	\dot{V}^\phi + \frac{1}{\tan\theta} \left( \dot\theta V^\phi + \dot\phi V^\theta \right) = 0.
\end{equation}
Suppose we parallel transport a vector along the equator with $\theta = \pi/2$, so $\dot\theta = 0$, but $\dot\phi \neq 0$.
Then $\dot{V}^\theta = \dot{V}^\phi = 0$, so $V^\theta = \text{const}$ and $V^\phi = \text{const}$.
By the symmetry of the sphere, the same result can be applied to a vector that is parallel transported along lines of constant longitude with $\dot\phi = 0$, but $\dot\theta \neq 0$.
In particular, if we carry a vector all the way around the triangle in \cref{fig:parallel_transport}, we find that the vector is rotated from its initial orientation.

The geodesic equation \eqref{eq:geodesic} becomes
\begin{equation}
	\ddot{\theta} - \sin\theta \cos \theta \, \dot\phi^2 = 0,
	\qquad
	\ddot\phi + \frac{2}{\tan \theta} \, \dot\theta \dot\phi = 0.
\end{equation}
Like above, we could specialize to $\dot\theta = 0$ and $\dot\phi \neq 0$ or $\dot\theta \neq 0$ and $\dot\phi = 0$ to find lines of constant latitude or longitude with $\ddot\theta=0$ or $\ddot\phi=0$, respectively.
By spherical symmetry, the geodesic between two points lies on a great circle connecting the points, as we expect from intuition.
But it is also possible to establish this result without relying on the symmetry.
First, note that the velocity magnitude
\begin{equation}
	\dot{x}^i \dot{x}_i = \left( \frac{\dif s}{\dif \tau} \right)^2 = r^2 \left( \dot\theta^2 + \sin^2 \theta \, \dot\phi^2 \right)
\end{equation}
is constant, for by substituting $\ddot\theta$ and $\ddot\phi$ we find
\begin{equation}
\begin{split}
	\odv*{\left( \dot\theta^2 + \sin^2 \theta \, \dot\phi^2 \right)}{\tau} &= 2 \dot\theta \ddot\theta + 2 \sin\theta \cos\theta \, \dot\theta \dot\phi^2 + 2 \sin^2\theta \, \dot\phi \ddot\phi \\
	                                                                       &= 2 \sin\theta \cos\theta \, \dot\theta \dot\phi^2 + 2 \sin\theta \cos\theta \, \dot\theta \dot\phi^2 - 4 \, \frac{\sin^2\theta}{\tan\theta} \, \dot\theta \dot\phi^2 = 0.
\end{split}
\end{equation}
We expect this, because the equation of parallel transport \eqref{eq:parallel_transport} with the metric connection \eqref{eq:def_christoffel} preserves magnitudes of vectors, and we derived the geodesic equation \eqref{eq:geodesic} by parallel transport of the tangent vector $\dot{x}^i$.
Should expect, because parallel transport preserves length and we derived the geodesic equation by parallel transport of the velocity $\dot{x}^i$.
We therefore set
\begin{equation}
	\dot\theta^2 + \sin^2\theta \, \dot\phi^2 = 1.
\label{eq:gr_summary:velmag}
\end{equation}
The second geodesic equation is
\begin{equation}
	\frac{\ddot\phi}{\dot\phi} - \frac{2}{\tan\theta} \, \dot\theta = 0
	\quad \text{or} \quad
	\odv*{\left( \log{\dot\phi} - 2 \log \sin \theta \right)}{\tau} = 0,
	\quad \text{so} \quad
	\dot\phi = \frac{C_1}{\sin^2\theta}.
\label{eq:gr_summary:phidotsol}
\end{equation}
Substituting this result into \cref{eq:gr_summary:velmag}, using the chain rule and taking the positive square root, we then have
\begin{equation}
	\odv\theta\tau = \odv\theta\phi \odv\phi\tau = \sqrt{\frac{\sin^2\theta - C_1^2}{\sin^2\theta}} .
\end{equation}
Using $\dot\phi$ from \cref{eq:gr_summary:phidotsol}, we find the separable equation
\begin{equation}
	\odv\phi\theta = \frac{C_1}{\sin\theta \sqrt{\sin^2\theta - C_1^2}}.
\end{equation}
Integrating this equation is possible, albeit not easy.
First, rewrite
\begin{equation}
	  \odv{\phi}{\theta}
	= \frac{C_1}{\sin\theta \sqrt{\sin^2\theta - C_1^2}}
	= \frac{C_1}{\sin^2\theta \sqrt{1 - C_1^2 / \sin^2 \theta}}
	= \frac{C_1}{\sin^2\theta \sqrt{1 - C_1^2 \left( 1 + 1/\tan^2\theta \right)}}.
\end{equation}
Now substitute $u = 1/\tan\theta$ with $\dif u = - \dif \theta / \sin^2\theta$ and integrate
\begin{equation}
\begin{aligned}
	\phi &= \int \frac{C_1 \, \dif \theta}{\sin\theta \sqrt{\sin^2\theta - C_1^2}} \\
	     &= -\int \frac{C_1 \, \dif u}{\sqrt{1 - C_1^2 \left( 1 + u^2 \right)}} = -\int \frac{\dif u}{\sqrt{\frac{1 - C_1^2}{C_1^2} - u^2}} \\
	     &= -\asin \left[ \frac{u}{\sqrt{(1-C_1^2)/C_1^2}} \right] + C = -\asin \left[ \frac{C_1 / \tan\theta}{\sqrt{1-C_1^2}} \right] + C.
\end{aligned}
\end{equation}
\iffalse
\begin{equation}
\begin{aligned}
	\int \odv\phi\theta &=  \int \frac{C_1 \, \dif \theta}{\sin\theta \sqrt{\sin^2\theta - C_1^2}} \\
	                    &=  \int \frac{C_1 \, \dif \theta}{\sin^2\theta \sqrt{1 - C_1^2 / \sin^2 \theta}} \\
	                    &=  \int \frac{C_1 \, \dif \theta}{\sin^2\theta \sqrt{1 - C_1^2 \left( 1 + 1/\tan^2\theta \right)}} \\
	                    &= -\int \frac{C_1 \, \dif u}{\sqrt{1 - C_1^2 \left( 1 + u^2 \right)}} \quad \left( u = 1/\tan\theta, \,\, \dif u = -\dif \theta / \sin^2 \theta \right) \\
	                    &= -\int \frac{\dif u}{\sqrt{\frac{1 - C_1^2}{C_1^2} - u^2}} \\
	                    &= -\asin \left[ \frac{u}{\sqrt{(1-C_1^2)/C_1^2}} \right] + C_2 \\
	                    &= -\asin \left[ \frac{C_1 / \tan\theta}{\sqrt{1-C_1^2}} \right] + C_2 \\
\end{aligned}
\end{equation}
\fi
By the trigonometric formula $\sin(A-B) = \sin A \cos B - \cos A \sin B$, we obtain the rather cryptic result
\begin{equation}
	\frac{C_1}{\tan \theta \sqrt{1 - C_1^2}} = \sin(C_2 - \phi) = \sin C_2 \cos \phi - \cos C_2 \sin \phi.
\end{equation}
To make sense of it, multiply by $\cos \theta$ and go back to Cartesian coordinates, where $x = r \sin\theta \cos\phi$, $y = r \sin\theta \sin\phi$ and $z = r \cos\theta$.
We then have the equation
\begin{equation}
	\frac{x}{r} \, \sin C_2 - \frac{y}{r} \, \cos C_2 - \frac{z}{r} \, \frac{C_1}{\sqrt{1 - C_1^2}} = 0.
\end{equation}
This is the equation of a plane $(\vec{r} - \vec{r}_0) \cdot \vec{n}$ with $\vec{n} = \sin C_2 \, \hat{\vec{x}} -\cos C \, \hat{\vec{y}} -C_1 / \sqrt{1 - C_1^2} \, \hat{\vec{z}}$ centered at the origin $\vec{r}_0 = \vec{0}$!
Thus, the geodesic between two points follow the intersection between this plane and the sphere, which is precisely the definition of a great circle.

Finally, let us investigate the curvature of the sphere.
The nonzero components of the Riemann curvature tensor \eqref{eq:def_riemann_tensor} are
\begin{equation}
	R\indices{^1_2_0_2} = \sin^2 \theta , \quad
	R\indices{^1_2_2_1} = -\sin^2 \theta , \quad
	R\indices{^2_1_1_2} = -1 , \quad
	R\indices{^2_1_2_1} = 1 .
\end{equation}
Then the nonzero components of the Ricci tensor \eqref{eq:def_ricci_tensor} are
\begin{equation}
	R_{00} = 1 , \qquad R_{11} = \sin^2 \theta ,
\end{equation}
and the Ricci curvature scalar \eqref{eq:def_ricci_scalar} is
\begin{equation}
	R = \frac{2}{r^2} .
\end{equation}
The radius $r$ is the only length scale we have defined, so up to the prefactor $2$, this is really the only result we can expect.
Since the coordinates $x^i$ are dimensionless and the metric tensor $g_{\mu \nu}$ proportional to $r^2$, we can use dimensional analysis to deduce that the Christoffel symbols \eqref{eq:def_christoffel}, Riemann tensor \eqref{eq:def_riemann_tensor} and Ricci tensor \eqref{eq:def_ricci_tensor} should all be dimensionless, so the Ricci scalar \eqref{eq:def_ricci_scalar} should be proportional to $1/r^2$.



\section{Least-action derivation of the Einstein field equations}
\label{sec:einstein_derivation}

Following \cite[section 4.3]{ref:carroll}, we will derive the Einstein field equations
\begin{equation}
	R_{\mu \nu} - \frac{1}{2} R g_{\mu \nu} = \frac{8 \pi G}{c^4} T_{\mu \nu}
\end{equation}
from the principle of least action.
We will \emph{postulate} the action
\begin{equation}
	S[g_{\mu \nu}, \nabla_\sigma g_{\mu \nu}] = \int \dif^n x \lagr(g_{\mu \nu}, \nabla_\sigma g_{\mu \nu})
	                                          = \int \dif^n x \sqrt{-\det{g}} \hat{\lagr}(g_{\mu \nu}, \nabla_\sigma g_{\mu \nu})
\end{equation}
that, when varied with respect to the metric $g_{\mu \nu}$ and subject to the principle of least action $\variation{S} = 0$, yields the Einstein field equations.
Here $\lagr$ and $\hat{\lagr}$ are Lagrangian densities with and without the metric determinant $\det{g} < 0$.
As the strategy simply involves \emph{guessing} the correct action that produces the desired equations, this derivation is not based on any physical first principles, so its consequences would ultimately have to be experimentally verified.
Nevertheless, \cite[page 160-161]{ref:carroll} explains how one can at the very least narrow down the choice of action based on scalar quantities that are relevant for describing curved space.

We postulate the \textbf{Hilbert action}
\begin{equation}
	% i have x = (ct, x1, x2, x3), so I have a c "already" in the first component
	% this is normal! see e.g. https://physics.stackexchange.com/a/322055/299916
	% remember [R] = 1/m^2
	S_H = \frac{c^3}{16 \pi G} \int \dif^n x \sqrt{-\det{g}} \, R .
	\label{eq:einstein_derivation:hilbert_action}
\end{equation}
As the Lagrangian is a scalar quantity and we showed that the simplest scalar quantity we could create is the Ricci scalar \eqref{eq:def_ricci_scalar}, it is not an unreasonable guess.
The prefactor has been conventiently chosen to yield correct result \eqref{eq:einstein_derivation:einstein_matter} in the end, which we saw in \cref{sec:einstein_to_poisson} led to Newtonian gravity in the Newtonian limit.
From an ignorant point of view, we could instead regard it as an arbitrary constant at this point, and eventually replace it with the right combination of constants that reproduce Newtonian gravity in the Newtonian limit.
We could get the corresponding equations of motion by plugging the Lagrangian density $\hat{\lagr} = R c^3 / 16 \pi G$ into the Euler-Lagrange equations
\begin{equation}
	\pdv{\hat{\lagr}}{\phi} - \nabla_\mu \left( \pdv{\hat{\lagr}}{{\left(\nabla_\mu \phi\right)}} \right) = 0 .
\end{equation}
In fact Hilbert himself did this \cite{ref:hilbert_from_lagrange}, but doing so requires a great deal of effort.
Instead, we will vary the action with respect to the metric and express the variation in the form 
\begin{equation}
	\variation{S_H} = \int \dif^n x \sqrt{-\det{g}} F(g_{\mu \nu}, \nabla_\sigma g_{\mu \nu}) \, \variation{g^{\mu \nu}} = 0 .
	\label{eq:einstein_derivation:action_form}
\end{equation}
Then we can conclude that the equations of motion are $F(g_{\mu \nu}, \nabla_\sigma g_{\mu \nu}) = 0$.

It may sound more natural to express the variation in terms of the ordinary metric $g_{\mu \nu}$ instead of its inverse $g^{\mu \nu}$, like we did above.
But since $g^{\mu \lambda} g_{\lambda \nu} = \delta^\mu_\nu$, varying both sides with the product rule relates the two by
\begin{equation}
	\variation{g_{\mu \nu}} = -g_{\mu \rho} g_{\nu \sigma} \variation{g^{\rho \sigma}} .
	\label{eq:einstein_derivation:var_g_ginv}
\end{equation}
Thus, the stationary points are the same regardless of which one we vary with respect to.
We vary with respect to the inverse metric, as it makes the derivation flow more naturally.

Using $R = R\indices{^\mu_\mu} = g^{\mu \nu} R_{\mu \nu}$ and varying the action \eqref{eq:einstein_derivation:hilbert_action} with the product rule, we obtain
\begin{equation}
	\variation{S_H} = \frac{c^3}{16 \pi G} \left(
	                  \underbrace{\int \dif^n x \sqrt{-\det{g}} \, g^{\mu \nu} \variation{R_{\mu \nu}}}_{\textstyle \variation{S}_1}
	                + \underbrace{\int \dif^n x \sqrt{-\det{g}} \, R_{\mu \nu} \variation{g^{\mu \nu}}}_{\textstyle \variation{S}_2}
	                + \underbrace{\int \dif^n x \, R \, \variation{\sqrt{-\det{g}}}                      }_{\textstyle \variation{S}_3}
					\right) .
%\begin{split}
%	                                                                                               \variation{S}_1 &= \int \dif^n x \sqrt{-\det{g}} g^{\mu \nu} \variation{R_{\mu \nu}} \\
%	\variation{S} = \variation{S}_1 + \variation{S}_2 + \variation{S}_3 , \quad \text{where} \quad \variation{S}_2 &= \int \dif^n x \sqrt{-\det{g}} R_{\mu \nu} \variation{g^{\mu \nu}} \\
%	                                                                                               \variation{S}_3 &= \int \dif^n x R \variation{\sqrt{-\det{g}}} \\
%\end{split}
%\begin{split}
%	\variation{S} &= \int \dif^n x \sqrt{-\det{g}} g^{\mu \nu} \variation{R_{\mu \nu}} \\
%	              &+ \int \dif^n x \sqrt{-\det{g}} R_{\mu \nu} \variation{g^{\mu \nu}} \\
%	              &+ \int \dif^n x R \variation{\sqrt{-\det{g}}} \\
%\end{split}
	\label{eq:einstein_derivation:ds_split}
\end{equation}
The second term $\variation{S}_2$ is already in the desired form \eqref{eq:einstein_derivation:action_form}, but we must do some work to bring $\variation{S}_1$ and $\variation{S}_3$ to the same form.

% TODO: latex package glossary?

First, let us take care of $\variation{S}_1$ by reexpressing $\variation{R_{\mu \nu}}$ in terms of metric variations in a top-down manner.
The Ricci tensor $R_{\mu \nu} = R\indices{^\lambda_\mu_\lambda_\nu}$ is the contraction of the Riemann tensor \eqref{eq:def_riemann_tensor}.
Varying it, we get
% TODO: do more intelligently by writing (\mu <-> \nu), etc.
\begin{equation}
	\variation{R\indices{^\rho_\sigma_\mu_\nu}} = \partial_\mu \variation{\Gamma^\rho_{\nu \sigma}}
	                                            - \partial_\nu \variation{\Gamma^\rho_{\mu \sigma}}
												+ \left(\variation{\Gamma^\rho_{\mu \lambda}}\right) \Gamma^\lambda_{\nu \sigma}
												+ \Gamma^\rho_{\mu \lambda} \left(\variation{\Gamma^\lambda_{\nu \sigma}\right)}
												- \left(\variation{\Gamma^\rho_{\nu \lambda}}\right) \Gamma^\lambda_{\mu \sigma}
												- \Gamma^\rho_{\nu \lambda} \left(\variation{\Gamma^\lambda_{\mu \sigma}\right)} .
	\label{eq:einstein_derivation:var_riemann}
\end{equation}
Now reexpress the variations of the Christoffel symbols.
Instead of hammering straight through their definition \eqref{eq:def_christoffel}, we observe that while single Christoffel symbols do not transform as a tensor, their \emph{variation} is the difference between two Christoffel symbols and \emph{do} \cite[page 96,98]{ref:carroll}.
It is therefore meaningful to use \cref{eq:def_cov_deriv} to take its covariant derivative
\begin{equation}
	\nabla_\lambda \variation{\Gamma^\rho_{\nu \mu}} = \partial_\lambda \variation{\Gamma^\rho_{\nu \mu}} 
	                                                 + \Gamma^\rho_{\lambda \sigma} \variation{\Gamma{^\sigma_{\nu \mu}}} 
	                                                 - \Gamma^\sigma_{\lambda \nu} \variation{\Gamma{^\rho_{\sigma \mu}}} 
	                                                 - \Gamma^\sigma_{\lambda \mu} \variation{\Gamma{^\rho_{\nu \sigma}}} .
	\label{eq:einstein_derivation:christoffel_cov_deriv}
\end{equation}
Flipping this equation around for $\partial_\lambda \variation{\Gamma^\rho_{\nu \mu}}$ and substituting the result into the variation of the Riemann tensor \eqref{eq:einstein_derivation:var_riemann}, we witness an avalanche of cancellations, leaving only the terms
\begin{equation}
	\variation{R\indices{^\rho_\mu_\lambda_\nu}} = \nabla_\lambda \variation{\Gamma^\rho_{\nu \mu}}
	                                             - \nabla_\nu \variation{\Gamma^\rho_{\lambda \mu}} .
\end{equation}
The variation of the Ricci tensor follows by contracting $\rho$ and $\lambda$. 
Then the first term in the variation of the action becomes
\begin{equation}
\begin{split}
	\variation{S}_1 &= \int \dif^n x \sqrt{-\det{g}} \, g^{\mu \nu} \left( \nabla_\lambda \variation{\Gamma^\lambda_{\mu \nu}} - \nabla_\nu \variation{\Gamma^\lambda_{\lambda \mu}} \right) \\
	                &= \int \dif^n x \sqrt{-\det{g}} \, \nabla_\sigma \left( g^{\mu \nu} \variation{\Gamma^\sigma_{\mu \nu}} - g^{\mu \sigma} \variation{\Gamma^\lambda_{\lambda \mu}} \right) . \\
	\label{eq:einstein_derivation:ds1_intermediate}
\end{split}
\end{equation}
We still have not brought the variation to the form \eqref{eq:einstein_derivation:action_form}, but it does not matter.
By \textbf{Stokes theorem} \cite[equation 3.35]{ref:carroll}
\begin{equation}
	\int_M \dif^n x \sqrt{\abs{g}} \nabla_\mu V^\mu = \int_{\partial M} \dif^{n-1} \sqrt{\abs{\gamma}} n_\mu V^\mu ,
\end{equation}
our integral for $\variation{S}_1$ over $n$-space can be converted into a boundary integral over $(n-1)$-space at infinity.
But the variational method that we have used here asserts that there is no variation on the boundary, so
\begin{equation}
	\variation{S}_1 = 0 .
\end{equation}

Let us now express $\variation{S}_3$ in terms of $\variation{g^{\mu \nu}}$.
We will need the matrix identity
\begin{equation}
	\log \, \det{M} = \trace \log M  .
	\label{eq:matrix_log_det_trace}
\end{equation}
This is trivial for diagonal matrices $M$.
By using the property $\det{AB} = \det{A} \det{B}$, we can easily extend it to diagonalizable matrices $M = P D P^{-1}$.
Varying both sides of \cref{eq:matrix_log_det_trace}, we obtain \cite{ref:matrix_ln_det_tr_exercise}
\begin{equation}
	\frac{\variation{\det{M}}}{\det{M}} = \trace(M^{-1} \variation{M}) .
\end{equation}
Taking $M$ to be the metric $g_{\mu \nu}$ and $M^{-1}$ its inverse $g^{\mu \nu}$, we find
\begin{equation}
	\variation{\det{g}} = \det{g} g^{\mu \nu} \variation{g_{\mu \nu}} = -\det{g} g_{\mu \nu} \variation{g^{\mu \nu}} ,
\end{equation}
where we used \cref{eq:einstein_derivation:var_g_ginv} to convert $\variation{g_{\mu \nu}}$ to $\variation{g^{\mu \nu}}$.
Now the chain rule gives
\begin{equation}
	\variation{\sqrt{-\det{g}}} = -\frac{1}{2} \frac{\variation{\det{g}}}{\sqrt{-\det{g}}} = -\frac{1}{2} \sqrt{-\det{g}} g_{\mu \nu} \variation{g^{\mu \nu}},
\end{equation}
so the third contribution to the variation of the action \eqref{eq:einstein_derivation:ds_split} is
\begin{equation}
	\variation{S}_3 = \int \dif^n x \sqrt{-\det{g}} \left( \frac{-1}{2} R g_{\mu \nu} \right) \variation{g^{\mu \nu}} .
\end{equation}

At last, we have brought the variation of the action to the form \eqref{eq:einstein_derivation:action_form} with
\begin{equation}
	\variation{S_H} = \frac{c^3}{16 \pi G} \int \dif^n x \sqrt{-\det{g}} \left( R_{\mu \nu} - \frac{1}{2} R g_{\mu \nu} \right) \variation{g^{\mu \nu}} = 0 .
\end{equation}
The variation of the integral can only vanish if the integrand vanishes, so we have found the \textbf{Einstein field equations in vacuum},
\begin{equation}
	 \frac{c^3}{16 \pi G} \frac{1}{\sqrt{-\det{g}}} \fdv{S_H}{g^{\mu \nu}} = R_{\mu \nu} - \frac{1}{2} R g_{\mu \nu} = 0 .
	\label{eq:einstein_derivation:einstein_vacuum}
\end{equation}
To unveil the Einstein field equations in the presence of matter, we add a contribution $S_M = \int \dif^n x \sqrt{-\det{g}} \, \lagr_M$ that represents matter to a new total action
\begin{equation}
	S = S_H + S_M .
\end{equation}
Repeating the same procedure as above yields the equations of motion
\begin{equation}
	\frac{c^3}{16 \pi G} \frac{1}{\sqrt{-\det{g}}} \fdv{S}{g^{\mu \nu}} = \left( R_{\mu \nu} - \frac{1}{2} R g_{\mu \nu} \right) + \frac{c^3}{16 \pi G} \frac{1}{\sqrt{-\det{g}}} \frac{\variation{S_M}}{\variation{g^{\mu \nu}}} = 0 .
\end{equation}
If we now \emph{define} the energy-momentum tensor
\begin{equation}
	T_{\mu \nu} = \frac{-c}{2 \sqrt{-\det{g}}} \frac{\variation{S_M}}{\variation{g^{\mu \nu}}} ,
\end{equation}
we uncover the \textbf{Einstein field equations in the presence of matter},
\begin{equation}
	R_{\mu \nu} - \frac{1}{2} R g_{\mu \nu} = \frac{8 \pi G}{c^4} T_{\mu \nu} .
	\label{eq:einstein_derivation:einstein_matter}
\end{equation}


\chapter{Relativistic fluid dynamics}

In this appendix, we will derive a number of important results from relativistic fluid mechanics needed for stability analysis of stars.
We will look at the \textbf{flow} of fluid elements along stream lines $x(\tau)$ in a \textbf{perfect fluid} with energy-momentum
\begin{equation}
	T_{\mu \nu} = \frac{1}{c^2} U_\mu U_\nu (\epsilon + P) - g_{\mu \nu} P .
\end{equation}

\section{Conservation of baryon number}

\TODO{make figure}

Due to both the geometry of spacetime and spatial change of velocities, the volume $V(x(\tau))$ of a fluid element changes as it moves along a streamline.
Let us derive the rate of change of this volume element in flat space, then generalize the result using the equivalence principle \TODO{ref}.
In flat space and in a frame that follows the fluid element, $U^\mu(x) \taylor (c, \vec{v}(x))$, where $\vec{v}(x)$ is small close to the fluid element.
As the fluid element flows from place to the next in a short time $\delta t = \delta \tau$, the length $L^i$ of the fluid element along a spatial dimension $i$ changes by
\begin{equation}
	\dif L^i = \Big[ v^i(x+L^i) - v^i(x) \Big] \dif \tau 
	         = \pdv{v^i(x)}{x^i} L^i \dif \tau.
\end{equation}
The volume of the fluid element then changes by
\begin{equation}
\begin{split}
	\dif V &= \dif \left( L^1 L^2 L^3 \right) \\
	       &= \dif \left( L^1 \right) L^2 L^3 + L^1 \left( \dif L^2 \right) L^3 + L^1 L^2 \left( \dif L^3 \right) \\
	       &= V \left( \frac{\dif L^1}{L^1} + \frac{\dif L^2}{L^2} + \frac{\dif L^3}{L^3} \right) \\
	       &= V \left( \pdv{v^1}{x^1} + \pdv{v^2}{x^2} + \pdv{v^3}{x^3} \right) \dif \tau \\
	       &= V \pdv{U^i}{x^i} \dif \tau .
\end{split}
\end{equation}
Since $U^0 = c$ in our reference frame, we make no mistake by including $\pdv{U^0}/{x^0} = 0$ in the sum.
The rate of change is then given by the tensorial law
\begin{equation}
	\odv{V}{\tau} = V \pdv{u^\alpha}{x^\alpha} = V \nabla_\mu U^\mu .
\end{equation}
By the equivalence principle and its tensorial transformation properties, the same law holds in any spacetime and in any reference frame.

Like the volume element, the baryon number density $n(x(\tau))$ can also change along a streamline.
In contrast, the total \textbf{baryon number} $N = n V$ must be conserved, as expressed by
\begin{equation}
	\odv*{\left( n V \right)}{\tau} = 0 .
\end{equation}
It will be very useful to rewrite this law in multiple different ways.
First, let us differentiate it with the product rule and insert the volume element rate of change \TODO{ref}.
Then the conservation law is equivalent to
\begin{equation}
	0 = \frac{1}{V} \odv*{\left( n U^\mu \right)}{\tau}
	  = \odv{n}{\tau} + \frac{n}{V} \odv{V}{\tau}
	  = U^\mu \nabla_\mu n + n \nabla_\mu U^\mu
	  = \nabla_\mu \left( n u^\mu \right) ,
\end{equation}
which has the elegant interpretation that there is no flux of the baryon number density current $n u^\mu$ out of the volume element.
We can use this result to rewrite the conservation law in a second way, namely
\begin{equation}
	\odv{n}{\tau} = \odv{x^\mu}{\tau} \pdv{n}{x^\mu}
	              = U^\mu \nabla_\mu n
	              = -n \nabla_\mu U^\mu .
\end{equation}

\section{Conservation of energy and Euler equation}

The conservation of energy-momentum $\nabla_\mu T^{\mu \nu} = 0$ implies
\begin{equation}
\begin{split}
	0 &= \nabla_\mu T^{\mu \nu} \\
	  &= \nabla_\mu \left[ \frac{1}{c^2} U_\mu U_\nu (\epsilon + P) - g_{\mu \nu} P \right] \\
	  &= \frac{1}{c^2} \bigg[ \Big( \epsilon + P \Big) \Big( U^\nu \nabla_\mu U^\mu + U^\mu \nabla_\mu U^\nu \Big) + U^\mu U^\nu \nabla_\mu \Big( \epsilon + P \Big) \bigg] - \nabla^\nu P . \\
\end{split}
\end{equation}

From this, it is possible to derive two separate results by \emph{projecting} the part of the vector $\nabla_\mu T^{\mu \nu}$ that is parallel and and orthogonal to $U^\mu$.
If we have two vectors $\vec{U}$ and $\vec{V}$, then we define the parallel and orthogonal projection of $\vec{V}$ on $\vec{U}$ by
\begin{equation}
	\vec{V}_\parallel = \left( \hat{\vec{U}} \cdot \vec{V} \right) \hat{\vec{U}} = \frac{\vec{U} \cdot \vec{V}}{\vec{U} \cdot \vec{U}} \vec{U}
	\qquad \text{and} \qquad
	\vec{V}_\perp = \vec{V} - \vec{V}_\parallel .
\end{equation}
Then $\vec{V}_\parallel + \vec{V}_\perp = \vec{V}$, $\vec{U} \cdot \vec{V} = \vec{U} \cdot \vec{V_\parallel}$ and $\vec{U} \cdot \vec{V}_\perp = 0$, so the naming makes sense.
With index notation, this becomes
\begin{equation}
	V_\parallel^\alpha = \frac{U_\beta V^\beta}{U_\mu U^\mu} \, U^\alpha
	\qquad \text{and} \qquad
	V_\perp^\alpha = V^\alpha - V_\parallel^\alpha = \left( \delta\indices{^\alpha_\beta} - \frac{U^\alpha U_\beta}{U_\mu U^\mu} \right) V^\beta .
\end{equation}
From these expressions, it is convenient to define the parallel and orthogonal \textbf{projection tensors}
\begin{equation}
	\left(P_\parallel\right) \indices{^\alpha_\beta} = \frac{U^\alpha U_\beta}{U^\mu U_\mu}
	\qquad \text{and} \qquad
	\left(P_\perp\right) \indices{^\alpha_\beta} = \left( \delta\indices{^\alpha_\beta} - \frac{U^\alpha U_\beta}{U^\mu U_\mu} \right)
\end{equation}
that project out the parallel part $V_\parallel^\alpha = \left(P_\parallel\right)\indices{^\alpha_\beta} V^\beta$ and $V_\perp^\alpha = \left(P_\perp\right)\indices{^\alpha_\beta} V^\beta$ of the vector $V^\alpha$.
These projectors are valid for any normalization $U^\mu U_\mu$.
With our conventions and choice of units, $U^\mu U_\mu = c^2$.

\subsection{Energy conservation}

First, let us see what the parallel part of $\nabla_\mu T^{\mu \nu} = 0$ gives us.
Instead of multiplying it my $\left(P_\parallel\right)\indices{^\alpha_\nu} = U^\alpha U_\nu /c^2$, however, let us only multiply by $U_\nu$.
Then we obtain
\begin{equation}
\begin{split}
	0 &= U_\nu \nabla_\mu T^{\mu \nu} \\
	  &= \frac{1}{c^2} \bigg[ \Big( \epsilon + P \Big) \Big( U_\nu U^\nu \nabla_\mu U^\mu + U^\mu U_\nu \nabla_\mu U^\nu \Big) + U^\mu U_\nu U^\nu \nabla_\mu \Big( \epsilon + P \Big) \bigg] - \nabla^\nu P . \\
\end{split}
\end{equation}
We can simplify this by using both $U^\nu U_\nu = c^2$ and its implication $\nabla_\mu \left( U^\nu U_\nu \right) = 2 U^\nu \nabla_\mu U^\nu = 0$.
This gives us
\begin{equation}
\begin{split}
	  0 &= \Big(\epsilon + P \Big) \nabla_\mu U^\mu + U^\mu \nabla_\mu \Big( \epsilon + P \Big) - U_\nu \nabla^\nu P \\
	    &= \nabla_\mu \bigg[ U^\mu \Big( \epsilon + p \Big)  \bigg] - U^\mu \nabla_\mu P . \\
\end{split}
\end{equation}
We can go even further if we apply the backwards product rule $U^\mu \nabla_\mu P = \nabla_\mu (U^\mu P) - P \nabla_\mu U^\mu$ to the last term.
Then we obtain
\begin{equation}
	0 = \nabla_\mu \bigg( \epsilon \, U^\mu \bigg) + P \, \nabla_\mu U^\mu .
\end{equation}
This is in fact the \textbf{relativistic equation of energy conservation}!
In the Newtonian limit \TODO{ref}, the pressure term is negligible and we obtain the familiar continuity equation
\begin{equation}
	0 = \nabla_\mu \bigg( \epsilon \, u^\mu \bigg) = \pdv{\epsilon}{t} - \vec{\nabla} \cdot \Big( \epsilon \vec{v} \Big) .
\end{equation}

It is useful to have the Euler equation in a form that involves derivatives $\odv{}/{\tau} = U^\mu \nabla_\mu$ along a streamline.
By repeated application of the product rule and substitution of baryon number conservation \TODO{ref}, we can rewrite \TODO{ref} as
\begin{equation}
\begin{split}
	0 &= U^\mu \nabla_\mu \epsilon + (\epsilon+P) \nabla_\mu U^\mu  \\
	  &= \odv{\epsilon}{\tau} + (\epsilon+P) \nabla_\mu U^\mu \\
	  &= \odv{\epsilon}{\tau} + \frac{\epsilon+P}{n} n \nabla_\mu U^\mu \\
	  &= \odv{\epsilon}{\tau} - \frac{\epsilon+P}{n} U^\mu \nabla_\mu n \\
	  &= \odv{\epsilon}{\tau} - \frac{\epsilon+P}{n} \odv{n}{\tau} . \\
\end{split}
\end{equation}
This is in the form we wanted.

\subsection{Euler equation}

Second, let us inspect the component of $\nabla_\mu T^{\mu \nu}$ orthogonal to $U^\mu$ using the orthogonal projection tensor $\left(P_\perp\right)\indices{^\alpha_\nu}$.
A straightforward calculation using the same tricks \TODO{ref} shows that
\begin{equation}
\begin{split}
	0 &=      \left(P_\perp\right)\indices{^\alpha_\nu} \nabla_\mu T^{\mu \nu} \\
	  &=      \left\{ \delta\indices{^\alpha_\nu} - \frac{U^\alpha U_\nu}{c^2} \right\} \\
	  &\times \Bigg \{ \frac{1}{c^2} \bigg[ \Big( \epsilon + P \Big) \Big( U^\nu \nabla_\mu U^\mu + U^\mu \nabla_\mu U^\nu \Big) + U^\mu U^\nu \nabla_\mu \Big( \epsilon + P \Big) \bigg] - \nabla^\nu P \Bigg\} \\
	  &=      \frac{1}{c^2} \bigg[ \Big( \epsilon + P \Big) \Big( U^\alpha \nabla_\mu U^\mu + U^\mu \nabla_\mu U^\alpha \Big) + U^\mu U^\alpha \nabla_\mu \Big( \epsilon + P \Big) \bigg] - \nabla^\alpha P \\
	  &-      \frac{1}{c^2} \bigg[ \Big( \epsilon + P \Big) U^\alpha \nabla_\mu U^\mu + U^\alpha U^\mu \nabla_\mu \Big( \epsilon + P \Big) - U^\alpha U^\nu \nabla_\nu P \bigg] \\
	  &=      \frac{1}{c^2} \Big( \epsilon + P \Big) U^\mu \nabla_\mu U^\alpha - \nabla^\alpha P + \frac{1}{c^2} U^\alpha U^\mu \nabla_\nu P .
\end{split}
\end{equation}
This is the \textbf{relativistic Euler equation}, for in the Newtonian limit \TODO{ref}, the three spatial indices reduce to the Euler equation
\TODO{also get gravitational acceleration in the perturbation?}
\begin{equation}
	\rho \left[ \pdv{\vec{v}}{t} + \left( \vec{v} \cdot \vec{\nabla} \right) \vec{v} \right] = -\nabla P .
\end{equation}


\subsection{Adiabadicity}

The flow of our perfect fluid is adiabatic, meaning there is no transfer of heat between fluid elements.
One way to understand this is that the energy-momentum tensor \TODO{ref} is diagonal, and any exchange of heat would have to come from energy flux terms, which are represented by off-diagonal elements.
A more verbose way is to consider the first law of thermodynamics,
\begin{equation}
	\dif E = \dif Q + \dif W = T \dif S - P \dif V .
\end{equation}
Consider a fluid element with volume $V$, internal energy $E = V \epsilon$, a constant number of particles $N = n V$ and entropy $s = S/N$ per particle.
Then the first law can be rewritten
\begin{equation}
	\dif \left( \epsilon \, \frac{N}{n} \right) = N T \dif s - P \dif \left( \frac{N}{n} \right) .
\end{equation}
Explicitly writing out the differentials and cancelling the constant $N$, we find
\begin{equation}
	\dif \epsilon = \frac{\epsilon + P}{n} \dif n + n T \dif s .
\end{equation}
To relate this to our flow, combine baryon number conservation \TODO{ref1} with energy conservation \TODO{ref2} into
\begin{equation}
	\odv{\epsilon}{\tau} = \frac{\epsilon + P}{n} \odv{n}{\tau} .
\end{equation}
This is the first law of thermodynamics with $\dif Q = T \dif S = 0$, showing that the flow indeed is adiabatic.

A useful consequence of adiabadicity is that it enables us to define the \textbf{adiabatic index}
\begin{equation}
	\gamma = \left( \pdv{\log (P/P_0)}{\log (n/n_0)} \right)_S
\end{equation}
at constant entropy, relative to some irrelevant pressure and number density scales $P_0$ and $n_0$.
In our case,
\begin{equation}
	\gamma = \frac{n}{P} \left( \pdv{P}{n} \right)_S
	       = \frac{n}{P} \frac{\odv{P}/{\tau}}{\odv{n}/{\tau}}
	       = \frac{\epsilon+P}{P} \odv{P}{\epsilon}
\end{equation}


\chapter{Matsubara energy summation}
\label{chap:matsum}

\textit{This appendix is based on reference \cite{ref:altland_simons}.}

When doing thermal field theory, one often encounters sums
\begin{equation}
	S = \sum_{n=-\infty}^{+\infty} s(E_n)
\label{eq:matsum:sum}
\end{equation}
of functions $s(E_n)$ over all \textbf{Matsubara energies}
\begin{equation}
	E_n = \begin{cases}
	          2 \pi n / \beta                        & \text{for bosons}    \\
	          2 \pi \left( n+\frac12 \right) / \beta & \text{for fermions} .\\
	      \end{cases}
\end{equation}
For example, we encountered sums in the form
\begin{equation}
	S = \sum_{n=-\infty}^{+\infty} \frac{1}{E_n^2 + E^2}
\label{eq:matsum:motivating_example}
\end{equation}
in \cref{eq:tft:matsubara_sum_bosons} and \cref{eq:tft:matsubara_sum_fermions}.
In this appendix we will demonstrate an elegant general method for computing such sums by contour integration in the complex plane.

First, we define the complex functions
\begin{equation}
	n_\pm(z) = \frac{1}{e^{\beta z} \mp 1}
	         = \begin{dcases}
		           \displaystyle \frac{1}{e^{\beta z} - 1} & \text{for bosons}     \\
		           \displaystyle \frac{1}{e^{\beta z} + 1} & \text{for fermions} . \\
	           \end{dcases}
\label{eq:matsum:distribution}
\end{equation}
Here and below, the upper and lower signs correspond to bosons and fermions, respectively.
This is the familiar Bose-Einstein distribution $n_+(z)$ for bosons and the Fermi-Dirac distribution $n_-(z)$ for fermions.
Importantly, they have simple poles at all imaginary Matsubara frequencies $z = i E_n$, as indicated by blue crosses in \cref{fig:matsum:contours}.

% Draw cross from https://tex.stackexchange.com/questions/123760/draw-crosses-in-tikz
\usetikzlibrary{decorations.markings}
\usetikzlibrary{shapes.misc}
\tikzset{cross/.style={cross out, draw=black, fill=none, minimum size=2*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt}, cross/.default={2pt}}

\def\r{1.65}
\def\w{0.21} % for circles
%\def\w{0.3} % for "pole"
\def\n{16}
\iffalse
\def\drawpoles{
	\path[blue, decoration={markings, mark=between positions 0.01 and 1 step 3mm with {\cross}}, postaction={decorate}] (0, -\r+\w/2) -- (0, +\r-\w/2) node [black, right=0.15cm, yshift=-0.5cm] {\scriptsize $i E_n$};
	\path[blue, decoration={markings, mark=between positions 0 and 1 step 1.0 with {\cross}}, postaction={decorate}] (-\r/2, 0) node[black, above] {\scriptsize $-E$} -- (+\r/2, 0) node [black, above] {\scriptsize $+E$};
}
\fi
\def\N{9}
\def\drawpoles{
\foreach \n in {1,2,...,\N} {
	\def\y{-\r + (\n-1)/(\N-1)*(2*\r)}
	\node[draw,cross] at (0,{\y}) {};
	%\draw[thick, red, decoration={markings, mark=at position 0.125 with {\arrow{>}}}, postaction={decorate}] (0,{\y}) circle [radius=\w];
}
\node[label={[label distance=3pt]right:{\scriptsize $iE_n$}}] at (0,\r/2) {};
\node[draw,cross,label=above:{\scriptsize $+E$}] at (+\r/2,0) {};
\node[draw,cross,label=above:{\scriptsize $-E$}] at (-\r/2,0) {};
}
\def\drawaxes{
	\draw[->, black!50!white, thin] (-1.38*\r, 0) -- (+1.38*\r, 0.0) node [above, black] {\scriptsize $\text{Re}(z)$};
	\draw[->, black!50!white, thin] (0, -1.38*\r) -- (0, +1.38*\r) node [right, black] {\scriptsize $\text{Im}(z)$};
}
\def\cross{
	\draw (-2pt,-2pt) -- (+2pt,+2pt);
	\draw (+2pt,-2pt) -- (-2pt,+2pt);
}

\begin{figure}
\centering
\begin{subfigure}{0.32\textwidth}
\centering
\begin{tikzpicture}
% TODO: avoid weird shifts in arrow placement? https://tex.stackexchange.com/questions/569658/pgfplots-put-decorative-arrow-center-at-the-specified-position
\drawaxes
\drawpoles
\iffalse
\draw[thick, red, decoration={markings, mark=at position 0.125 with {\arrowreversed{>}}}, postaction={decorate}] (0, 0) circle [radius=(\r+\w)];
\draw[thick, red!50!white, decoration={markings, mark=at position 0.83 with {\arrow{>}}}, postaction={decorate}] 
      (+\w, -\r) -- 
      (+\w, +\r) arc [start angle=0, end angle=180, radius=\w] --
      (-\w, -\r) arc [start angle=180, end angle=360, radius=\w];
\fi
\foreach \n in {1,2,...,\N} {
	\def\y{-\r + (\n-1)/(\N-1)*(2*\r)}
	\draw[thick, red, decoration={markings, mark=at position 0.125 with {\arrow{>}}}, postaction={decorate}] (0,{\y}) circle [radius=\w];
}
\end{tikzpicture}
\caption{\label{fig:matsum:contourA}Contour $C^A_N$}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\begin{tikzpicture}
\drawaxes
\drawpoles
\iffalse
\draw[semithick, red!50!white, decoration={markings, mark=at position 0.4 with {\arrow{latex}}}, postaction={decorate}] 
      (+\w/2, -\r) -- 
      (+\w/2, +\r) arc [start angle=0, end angle=180, radius=\w/2] --
      (-\w/2, -\r) arc [start angle=180, end angle=360, radius=\w/2];
\fi
\foreach \n in {1,2,...,\N} {
	\def\y{-\r + (\n-1)/(\N-1)*(2*\r)}
	\draw[thick, red!50!white, decoration={markings, mark=at position 0.125 with {\arrow{>}}}, postaction={decorate}] (0,{\y}) circle [radius=\w];
}
\draw[thick, red, decoration={
	markings,
	mark=at position 0.125 with {\arrowreversed{>}},
}, postaction={decorate}] (0, 0) circle [radius=\r+\w];
\end{tikzpicture}
\caption{\label{fig:matsum:contourB}Contour $C^B_N$}
\end{subfigure}
\begin{subfigure}{0.32\textwidth}
\centering
\begin{tikzpicture}
\drawaxes
\drawpoles
\draw[
	decoration={
		markings, 
		mark=at position 0.6 with {\arrow{>}},
	}, postaction={decorate}, 
	thick, red
] (+\w, {-\r}) -- (+\w, {+\r}) arc [start angle=+90, end angle=-90, radius={\r}];
\draw [
	decoration={
		markings, 
		mark=at position 0.9 with {\arrow{>}},
	}, postaction={decorate}, 
	thick, red
] (-\w, {+\r}) -- (-\w, {-\r}) arc [start angle=270, end angle=90, radius={\r}];
\end{tikzpicture}
\caption{\label{fig:matsum:contourC}Contour $C^C_N$}
\end{subfigure}
\caption{\label{fig:matsum:contours}%
	To evaluate a Matsubara energy sum $S_N = \sum_{n=-N}^N s(E_n)$ in the limit $N \rightarrow \infty$, we can use the residue theorem to transform it into a complex integral $\oint \dif z \, s(-iz) n_\pm(z)$ along the contour $C^A_N$ that encloses some of the infinitely many poles of $n_\pm(z)$.
	Then we add the integral along the circular contour $C_N^B$, resulting in the equivalent contour $C^C_N$.
	If the integrand vanishes on $C_N^B$ as $N \rightarrow \infty$, we can trade $C_N^A$ for $C_N^C$ to transform the \emph{infinite} Matsubara sum into a sum over the assumed \emph{finite} number of residues of $s(-iz)$ in the real half planes.
}
\end{figure}

For reasons that will soon be clear, consider the contour integral
\begin{equation}
	\oint_{C_N^A} \dif z \, s(-iz) n_\pm(z)
\label{eq:matsum:contour_integral}
\end{equation}
along the contour $C_N^A$ drawn in \cref{fig:matsum:contourA} that encircle the $2N+1$ poles of $n_\pm(z)$ that are closest to the origin.
To proceed in a mathematically well-defined way, we consider only a finite number of poles for now, but will include all poles by taking the limit $N \rightarrow \infty$ at the very end of this derivation.
Otherwise, we would have to argue that $C_\infty^A$ can be closed at infinity, if it were to include \emph{all} poles of $n_\pm(z)$.
Our strategy will circumvent this difficulty.

How does this contour integral relate to the Matsubara sum \eqref{eq:matsum:sum}?
First recall the residue theorem
\begin{equation}
	\oint_C \dif z \, f(z) = 2 \pi i \sum_n \res_{z=z_n} [f(z)] ,
\label{eq:matsum:residue_theorem}
\end{equation}
where the sum runs over all poles $z_n$ of $f(z)$ inside the region enclosed by the contour $C$.
Second, recall that the residue of a quotient function $f(z) / g(z)$ at a simple pole $z = z_0$ where $g(z_0) = 0$, but $g'(z_0) \neq 0$ is 
\begin{equation}
	\res_{z=z_0} \left[ \frac{f(z)}{g(z)} \right] = \frac{f(z_0)}{g'(z_0)} .
\label{eq:matsum:residue_quotient}
\end{equation}
Let us apply this to our integral \eqref{eq:matsum:contour_integral} with $f(z) = s(-iz)$ and $g(z) = n_\pm(z)^{-1} = e^{\beta z} \mp 1$.
We then find that the residues of the integrand in the contour integral \eqref{eq:matsum:contour_integral} are
\begin{equation}
	\res_{z = i E_n} [s(-iz) n_\pm(z)] = \pm \frac{1}{\beta} s(E_n) ,
\end{equation}
By the residue theorem \eqref{eq:matsum:residue_theorem}, the contour integral \eqref{eq:matsum:contour_integral} is then
\begin{equation}
	\oint_{C_N^A} \dif z \, s(-iz) n_\pm(z) = 2 \pi i \sum_{n=-N}^N \res_{z = i E_n}[s(-i z) n_\pm(z)]
	                                  = \pm \frac{2 \pi i}{\beta} \sum_{n=-N}^N s(E_n) .
\label{eq:matsum:connection_integral_sum}
\end{equation}
In other words, the contour integral gives the finite Matsubara sum
\begin{equation}
	S_N = \sum_{n=-N}^N s(E_n) = \pm \frac{\beta}{2 \pi i} \oint_{C_N^A} \dif z \, s(-iz) n_\pm(z) ,
\label{eq:matsum:sum_as_contour_integral}
\end{equation}
and the full sum \eqref{eq:matsum:sum} is given by the limit $S = S_\infty = \lim_{N \rightarrow \infty} S_N$.

What is the use of transforming a simple sum into a complex (in both senses) contour integral?
Consider now the circular contour $C_N^B$ in \cref{fig:matsum:contourB} that touches $C_N^A$ at the uppermost and lowermost intersections with the imaginary axis.
Now
\begin{equation}
	\oint_{C_N^A} \dif z +
	\oint_{C_N^B} \dif z =
	\oint_{C_N^C} \dif z ,
\label{eq:matsum:contour_superposition}
\end{equation}
where $C_N^C$ is the contour shown in \cref{fig:matsum:contourB}.
The overlapping circular parts of $C_N^A$ run opposite ways and cancel, producing the vertical parts of $C_N^C$.
The clockwise circular contour $C_N^B$ cancels the top and bottom part of $C_N^A$, and also closes the contour with semicircles in the real half planes.

Finally, include all poles by taking the limit $N \rightarrow \infty$.
This sends the radii of the two semicircles to infinity, inflating the contour $C_N^C$ to the full real half planes, except the imaginary axis $\real z = 0$.
Assume now that $s(-i z)$ -- as in our motivating example \eqref{eq:matsum:motivating_example} -- satisfies
\begin{equation}
	\abs{s(-iz) n_\pm(z)} < \frac{1}{\abs{z}} \quad \text{as} \quad \abs{z} \rightarrow \infty .
\end{equation}
Our motivating example \eqref{eq:matsum:motivating_example} satisfies this.
For general $s(-iz)$, this assumption is not as restrictive as it may sound, since there must be some bound on $s(-iz)$ anyway if the sum \eqref{eq:matsum:sum} is to converge at all.
With this assumption, $\oint_{C_B} \dif z \, s(-iz) n_\pm(z) \rightarrow 0$ as $N \rightarrow \infty$, and by \cref{eq:matsum:contour_superposition} we can then trade the contour $C_N^A$ for $C_N^C$ in \cref{eq:matsum:sum_as_contour_integral}.

There is a big benefit to exchanging the contour $C_N^A$ for $C_N^C$.
We can now apply the residue theorem again with the new contour $C_\infty^C$ to obtain
\begin{equation}
	S = \frac{\pm \beta}{2 \pi i} \oint_{C_\infty^C} \dif z \, s(-iz) n_\pm(z) = \mp \beta \sum_n \res_{z=z_n} [s(-iz) n_\pm(z)] ,
\label{eq:matsum:sum_finite_residues}
\end{equation}
\emph{where the sum now runs over the poles of $s(-iz)$ in the real half planes, and not the poles of $n_\pm(z)$ like in \cref{eq:matsum:connection_integral_sum}}.
Note also that the sign change following the clockwise orientation of $C_\infty^C$.
Like in our example \eqref{eq:matsum:motivating_example}, most $s(-iz)$ will have a \emph{finite} number of poles, in stark contrast to the infinite number of poles of $n_\pm(z)$.
We have thus transformed the Matsubara sum over an infinite number of residues of $n_\pm(z)$ to a sum over a finite number of residues of $s(-iz)$ -- a much more manageable task.
This is our proclaimed elegant and general method of evaluating the Matsubara sum \eqref{eq:matsum:sum}.

\textbf{Example:}
Let us use now use this technique to evaluate the sum \eqref{eq:matsum:motivating_example} with
\begin{equation}
	s(-iz) = \frac{1}{-z^2 + E^2} .
\end{equation}
It has poles at $z = E$ and $z = -E$.
Using \cref{eq:matsum:residue_quotient} with $f(z) = n_\pm(z)$ and $g(z) = f(-iz)^{-1}$, we obtain the residues
\begin{equation}
	\res_{z =  E}[s(-iz) n_\pm(z)] = -\frac{1}{2 E} n_\pm(+E) 
	\qquad \text{and} \qquad
	\res_{z = -E}[s(-iz) n_\pm(z)] =  \frac{1}{2 E} n_\pm( E) .
\end{equation}
Using our main result \eqref{eq:matsum:sum_finite_residues}, the Matsubara sum \eqref{eq:matsum:motivating_example} is therefore
\begin{equation}
	S = \sum_{n=-\infty}^{+\infty} \frac{1}{E_n^2 + E^2}
	  = \mp \beta \sum_i \res_{z=z_n}[s(-i z) n_\pm(z)]
	  = \mp \frac{\beta}{2 E} \left[ n_\pm(-E) - n_\pm(E) \right] .
\end{equation}
We can now evaluate the sum explicitly by inserting the Bose-Einstein distribution and the Fermi-Dirac distribution \eqref{eq:matsum:distribution}.
After some simplification, we obtain
\begin{equation}
	S = \sum_{n=-\infty}^{+\infty} \frac{1}{E_n^2 + E^2}
	  = \frac{\beta}{2 E} \left( 1 \pm \frac{2}{e^{\beta E} \mp 1} \right)
	  = \frac{\beta}{2 E} \left[ 1 \pm 2 n_\pm(E) \right] .
\label{eq:matsum:example_result}
\end{equation}
For later reference, recall that the upper and lower signs hold for bosons and fermions, respectively.

\chapter{Integrals}

\newcommand\formulawithcomment[4]{%
\textbf{#1:}
#2
\textbf{#3:} #4
}

\newcommand\formulawithproof[3]{\formulawithcomment{#1}{#2}{Proof}{#3}}
\newcommand\formulawithreference[3]{\formulawithcomment{#1}{#2}{Reference}{#3}}

\formulawithproof{Scaled Gaussian integral}{
	\begin{equation}
		\int_{-\infty}^{+\infty} \dif x \, e^{-x^2} = \sqrt{\pi}
	\label{eq:integrals:gaussian_sqrtpi}
	\end{equation}
}{
	Convert to spherical coordinates $(r, \theta)$ and take the square of the two-dimensional integral
	\begin{equation*}
	\begin{split}
			\left[ \int_{-\infty}^{+\infty} \dif x \, e^{-x^2} \right]^2 &= \int_{-\infty}^{+\infty} \dif x \int_{-\infty}^{+\infty} \dif y \, e^{-(x^2 + y^2)} \\
		                                                             &= \int_{0}^{2 \pi} \dif \theta \int_{0}^{\infty} \dif r \, r e^{-r^2} \\
																	 &= 2 \pi \left[ -\frac12 e^{-r^2} \right]_{r=0}^{r=\infty} = \pi . \\
	\end{split}
	\end{equation*}
}

\formulawithproof{Rescaled Gaussian integral}{
	\begin{equation}
	\int_{-\infty}^{+\infty} \dif x \, e^{-ax^2} = \sqrt{\frac{\pi}{a}}
	\label{eq:integrals:gaussian_axx}
	\end{equation}
}{
	Change the integration variable to $y = \sqrt{a} x$ with $\dif y = \sqrt{a} \dif x$ and use integral \eqref{eq:integrals:gaussian_sqrtpi} to obtain
	\begin{equation*}
		\int_{-\infty}^{+\infty} \dif x \, e^{-ax^2} = 
		\int_{-\infty}^{+\infty} \frac{\dif y}{\sqrt{a}} \, e^{-y^2} = 
		\sqrt{\frac{\pi}{a}} .
	\end{equation*}
}

\formulawithproof{Gaussian integral over two Grassmann numbers}{
	\begin{equation}
		\int \dif \conj\psi \int \dif \psi \, e^{-\conj\psi a \psi} = a
		\qquad \text{for real $a$ and Grassmann numbers $(\psi, \conj\psi)$} .
	\label{eq:integrals:gaussian_grassmann_one}
	\end{equation}
}{
	Using the Taylor expansion \eqref{eq:gnums:exponential_taylor_series}, and the Grassmann number integral definitions \eqref{eq:gnums:integration_const} and \eqref{eq:gnums:integration_var} and the anticommutator \eqref{eq:gnums:anticommutators},
	\begin{equation*}
	\begin{split}
		\int \dif \conj\psi \int \dif \psi \, e^{-\conj\psi a \psi} = \int \dif \conj\psi \int \dif \psi \, (1 - \conj\psi a \psi)
		                                                            = a \int \dif \conj\psi \int \dif \psi \, \psi \conj\psi = a .
	\end{split}
	\end{equation*}
}

\formulawithproof{Gaussian integral over multiple pairs of Grassmann numbers}{
	\begin{equation}
		\int \dif \psi^\dagger \int \dif \psi \, e^{-\psi^\dagger A \psi} = \tdet A
		\qquad \text{for Hermitean $A$ and Grassmann numbers $(\psi, \psi^\dagger)$} .
	\label{eq:integrals:gaussian_grassmann_multiple}
	\end{equation}
}{
	The Hermitean matrix $A$ can be diagonalized by a unitary transformation $U$ as $A = U^\dagger D U$, where $D$ is a diagonal matrix with the eigenvalues of $A$ on its diagonal.
	Thus, 
	\begin{equation*}
		\psi^\dagger A \psi = \psi^\dagger U^\dagger D U \psi = (U \psi)^\dagger D (U \psi) = \tilde\psi^\dagger D \tilde\psi, \quad \text{where } \tilde\psi = U \psi.
	\end{equation*}
	The unitary transformation has $\abs{\tdet U} = 1$, so the integration measure $\dif \psi = \dif \tilde\psi$ is unchanged upon changing variables.
	A Hermitean matrix has real eigenvalues, so we can use integral \eqref{eq:integrals:gaussian_grassmann_one} to show that
	\begin{equation*}
	\begin{split}
		\int \dif \psi^\dagger \int \dif \psi \, e^{-\psi^\dagger A \psi} &= \int \dif \tilde\psi^\dagger \int \dif \tilde\psi \, e^{-\tilde\psi^\dagger D \tilde\psi} \\
		                                                                  &= \prod_i \int \dif \tilde\psi^*_i \int \dif \tilde\psi_i \, e^{-\tilde\psi^*_i \lambda_i \tilde\psi_i} \\ 
																		  &= \prod_i \lambda_i = \tdet A . \\
	\end{split}
	\end{equation*}
}

\chapter{Code}

\section{Derivation of the Tolman-Oppenheimer-Volkoff equation \texorpdfstring{\\}{} without using energy-momentum conservation}
\label{sec:tov_cas_derivation}

When deriving \cref{eq:tov} analytically, we made use of energy-momentum conservation $\nabla_\mu T\indices{^\mu^\nu} = 0$ instead of substituting our results into the unused \cref{eq:einstein_to_tov:thetatheta}.
Here, we do the latter in the computer algebra system SAGE, inspired by \cite{ref:sage_tov}.

\codefile{python}{../code/tov_derive.sage}

The output matches \cref{eq:tov} precisely.

\chapter{Numerical integration of the Tolman-Oppenheimer-Volkoff equation}
\label{sec:nstars:numtov}

In \cref{chap:tov} we derived the Tolman-Oppenheimer-Volkoff system \eqref{eq:tov:tovsys} for the unknown pressure and mass profiles $P(r)$ and $m(r)$, subject to the boundary conditions $P(0) = P_0$ and $m(0) = 0$.
Explicitly inserting the equation of state \eqref{eq:tov:tovsys_eos} into the mass and pressure gradients \eqref{eq:tov:tovsys_pressure} and \eqref{eq:tov:tovsys_mass}, the system reads
\begin{subequations}
\label{eq:numtov:tovsys}
\begin{align}
	\odv{P}{r} &= -\frac{G m \epsilon(P)}{r^2 c^2} \left[ 1 + \frac{P}{\epsilon(P)} \right] \left[ 1 + \frac{4 \pi r^3 P}{m c^2} \right] \left[ 1 - \frac{2 G m}{r c^2} \right]^{-1} , \label{eq:numtov:tov_pressure} \\
	\odv{m}{r} &= \frac{4 \pi r^2 \epsilon}{c^2} . \label{eq:numtov:tov_mass}
\end{align}
\end{subequations}
This is a system of two differential equations in the form $\odv{\vec{y}}/{t} = f(t, \vec{y})$ with $t=r$ and $\vec{y} = [P, m]$ that is suitable for Runge-Kutta integration algorithms.
In the following, we show how to integrate this system numerically.

To avoid issues with numerical instability, it is wise to eliminate physical scales from the system and introduce dimensionless variables that can be kept as close to $1$ as possible.
Define the dimensionless variables
\begin{equation}
	\diml{\epsilon}(r) = \frac{\epsilon(r)}{\epsilon_0}, \quad
	\diml{P}(r) = \frac{P(r)}{\epsilon_0}, \quad
	\diml{m}(r) = \frac{m(r)}{m_0} \quad \text{and} \quad
	\diml{r}(r) = \frac{r}{r_0},
\label{eq:numtov:dimensionless_variables}
\end{equation}
where $m_0$ and $r_0$ are two natural scales of stellar mass \TODO{masses?} and radius \TODO{radii?}, and
\begin{equation}
	\epsilon_0 = \frac{m_0 c^2}{4 \pi r_0^3 / 3}
	%m_0 = \text{solar mass ?}, \quad
	%r_0 = \text{10 km ?}.
\label{eq:numtov:energy_density_scale}
\end{equation}
is a corresponding natural scale of energy density.
For a neutron star, it would be appropriate to choose the solar mass $m_0 = \solarmass$ and $r_0 = \SI{10}{\kilo\meter}$, for example.
In short, any variable that wears a hat $\hat{}$ is the dimensionless version of its hatless sibling.
With the dimensionless variables \eqref{eq:numtov:dimensionless_variables}, the TOV system \eqref{eq:numtov:tovsys} becomes
\begin{subequations}
\label{eq:numtov:tovsys_dimless_complicated}
\begin{align}
	\odv{\diml{P}}{\diml{r}} &= -\frac{G m_0}{r_0 c^2} \frac{\diml{m} \diml{\epsilon}(\diml{P})}{\diml{r}^2} \left[ 1 + \frac{\diml{P}}{\diml{\epsilon}(\diml{P})} \right] \left[ 1 + \frac{4 \pi r_0^3 \epsilon_0}{m_0 c^2} \frac{\diml{r}^3 \diml{P}}{\diml{m}} \right] \left[ 1 - \frac{2 G m_0}{r_0 c^2} \frac{\diml{m}}{\diml{r}} \right]^{-1} , \\
	\odv{\diml{m}}{\diml{r}} &= \frac{4 \pi r_0^3 \epsilon_0}{m_0 c^2} \diml{r}^2\diml{\epsilon}(\diml{P}) .
\end{align}
\end{subequations}
First, note that the energy density scale \eqref{eq:numtov:energy_density_scale} further simplifies $4 \pi r_0^3 \epsilon_0 / m_0 c^2 = 3$.
Second, observe that $G_0 = r_0 c^2 / m_0$ has the same units as the gravitational constant $G$, so let us also introduce the dimensionless gravitational constant
\begin{equation}
	\diml{G} = \frac{G}{G_0} .
\end{equation}
With these observations, the system \eqref{eq:numtov:tovsys_dimless_complicated} simplifies to
\begin{subequations}
\label{eq:numtov:tovsys_dimless_simple}
\begin{align}
	\odv{\diml{P}}{\diml{r}} &= - \frac{\diml{G} \diml{m} \diml{\epsilon}(\diml{P})}{\diml{r}^2} \left[ 1 + \frac{\diml{P}}{\diml{\epsilon}(\diml{P})} \right] \left[ 1 + \frac{3 \diml{r}^3 \diml{P}}{\diml{m}} \right] \left[ 1 - \frac{2 \diml{G} \diml{m}}{\diml{r}} \right]^{-1} , \\
	\odv{\diml{m}}{\diml{r}} &= 3 \diml{r}^2 \diml{\epsilon}(\diml{P}) .
\end{align}
\end{subequations}
which we wish to solve subject to the boundary conditions $\diml{P}(0) = \diml{P}_0$ and $\diml{m}(0) = 0$ for some dimensionless central pressure $\diml{P}_0$.

To find the mass and radius of some star with central pressure $\diml{P}_0$, we can integrate the system \eqref{eq:numtov:tovsys_dimless_simple} until $\diml{P} \le 0$.
We then terminate the integration algorithm and call the final radius $\diml{r} = \diml{R}$ and mass $\diml{m}(\diml{r}) = \diml{M}$ the radius and mass of the star.
By parametrizing multiple stars with a range of central pressures $\diml{P}_0$ and performing this task for each of them, we obtain a mass-radius relation of the star.

Below is a small Python program that accomplishes all of this for an arbitrary equation of state $\diml\epsilon = \diml\epsilon(\diml{P})$.
The function \verb|massradiusplot| can be run with the optional parameter \verb|visual=True| to show the mass-radius relation in real-time, updating it every time the mass and radius of a new star is found.
Typically, stars distributed uniformly between two central pressures may be located very non-uniformly in the mass-radius space.
To circumvent this difficulty and make the mass-radius curve as smooth as possible, we take an adaptive approach by recursively splitting an initial central pressure interval $(\diml{P}_1, \diml{P}_2)$ until the Euclidean distance $\sqrt{(\diml{R}_2-\diml{R}_1)^2 + (\diml{M}_2-\diml{M}_1)^2}$ between all points on the curve is below some given tolerance.

\codefile{python}{../code/tov_solve.py}

\section{Cold free fermion neutron stars}

Below, we use the general framework above to find the mass-radius relation for the equations of state for cold free fermion neutron stars that we found in \cref{sec:nstars:nr_limit} and \cref{sec:nstars:gr_limit}.

\codefile{python}{../code/tov_free_fermi_gas.py}
